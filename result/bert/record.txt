测试不同lr: √   （由于lr比较敏感，且数据量不同对Test ACC影响较大，故使用val acc

测试不同结构： √  val acc
 - 一层
 - 两层
 - 加ReLU

测试不同数据清洗方法在最好的那一组的性能  √


针对错误句子，如何改进：只能考虑数据微调 ？？？ -> TODO   考虑去掉 ' no '，作为case study，不同组均有提升
可以找一些句子，去掉no对理解句子没有影响（）
可以研究前后哪个变对，哪个变错

不同维度看看SNE效果（32 64 128 256 512 -> 32最好 TODO SNE 

注意不同数据要格式一致，好用一份代码调用，new_washed_data重跑一下

目前最好的使用数据量5000_BERTmy训出来的
test 86.6

Testing Accuracy: 0.8663, Precision: 0.8702, Recall: 0.8663, F1: 0.8661

去掉 ' no '后：
Testing Accuracy: 0.8719, Precision: 0.8750, Recall: 0.8719, F1: 0.8717
说明Bert可能并不依赖显性的否定词，而更倾向于分析语境，当带有明显的否定倾向表示转折时，模型往往难以学习到这种特征
当去掉例如no这样的词后，我们的模型反而能够学习到真实的语境  or no 这种词对语境的主导性太强，可能误导模型


目前最好: 32维度  （两层，无ReLU
Testing acc: 8719 & 8747
Testing Accuracy: 0.8747, Precision: 0.8774, Recall: 0.8747, F1: 0.8745