# CS3307 Internet Information Extraction

## Introduction 介绍
情感分析是一种文本分析方法，利用自然语言处理技术从文本中提取信息和语义，确定作者的态度。这包括抽取实体、属性、观点、观点持有者、时间等要素，以分析文本对商品评价、明星舆论等的情感。

本项目使用推特评论数据（160万条训练集，360条测试集），采用K近邻、支持向量机、朴素贝叶斯和随机森林等传统机器学习模型，以不同方式识别文本情感倾向。同时，尝试了使用Transformer的Bert模型，并对其进行了改进以在理解语境和语义信息方面表现出色。最后，项目还探索了大语言模型ChatGPT，并利用prompt engineering技术引导模型生成更准确的情感分析结果，同时研究了不同数据清洗方法对分类效果的影响。


## Requirement 项目环境

```bash
pip install -r requirement.txt
```

## Run 项目运行

### 准备

#### 模型准备
在huggingface下载`bert-base-uncased`文件，包括
- config.json
- pytorch_model.bin
- tokenizer_config.json
- tokenizer.jon
- vocab.txt

放在`projectxx/`目录下，即与`projectxx/src`同级目录。

#### 数据准备
将训练数据改名为train.csv，测试数据改为test.csv，放在`./src/data目录中`，更改`./src/data/wash_data_base.py`参数后运行`wash_data_base.py`（运行此文件前需要配置nltk库的punkt工具）即可得到清洗后的数据`washed_train_data.csv`与`washed_test_data.csv`。

### 传统机器学习方法

进入`./src/traditional_ML`，使用`KNN, Naive bayes, random forest, svm`四种机器学习方法，运行指令：

```bash
python KNN.py 
python Naive_bayes.py
python random_forest.py
python svm.py
```

### 深度学习方法
进入`./src/DL`，使用预训练语言模型Bert，添加两层线性层进行微调训练，运行指令

#### 模型训练

```bash
python BERT.py --gpu [gpu_id]
```

#### 模型测试

如果你想直接测试本项目训练出的最高性能模型，请直接运行
```bash
python BERT.py --gpu [gpu_id] --test-only
```
运行这一步时，请保证`32-best_model.pth`文件在同级目录下。

最高性能： `Testing Accuracy: 0.8802, Precision: 0.8823, Recall: 0.8802, F1: 0.8801`

### 大语言模型方法
进入`./src/LLM`，使用`GPT-3.5/4`进行文本分类。

自行配置`OPEN_API_KEY`等信息后即可运行文件`openai-test.py`

`GPT-4`文本分类性能：`Testing Accuracy: 0.9387186629526463 Precision: 0.9081632653061225 Recall: 0.978021978021978 F1 score: 0.9417989417989417`

### project structures TODO update
```
.
├── README.md
├── requirements.txt
├── result
│   ├── GPT_3.5
│   │   └── GPT35.txt
│   ├── GPT_4
│   │   └── GPT4.txt
│   ├── SNE res
│   │   ├── 128best.png
│   │   ├── 256best.png
│   │   ├── 32best.png
│   │   ├── 64best.png
│   │   └── output.png
│   ├── bert
│   │   ├── 1-lr=1e-5.txt
│   │   ├── 1-lr=2e-5.txt
│   │   ├── 1-lr=3e-5.txt
│   │   ├── 1-lr=4e-5.txt
│   │   ├── 1-lr=5e-5.txt
│   │   ├── 2-BERTmy_lr=4e-5_two_layers_NO_ReLU.txt
│   │   ├── 2-BERTmy_lr=4e-5_two_layers_With_ReLU.txt
│   │   ├── 3-128max-size_washed_data.txt
│   │   ├── 3-256max-size_washed_data.txt
│   │   ├── 3-32max-size_washed_data.txt
│   │   ├── 3-64max-size_washed_data.txt
│   │   ├── 4-1e-5_new_washed_data.txt
│   │   ├── 4-2e-5_new_washed_data.txt
│   │   ├── 4-3e-5_new_washed_data.txt
│   │   ├── 4-4e-5_new_washed_data.txt
│   │   ├── 4-5e-5_new_washed_data.txt
│   │   ├── 5-关于去掉no这个词.txt
│   │   ├── 5000_with_embedding.txt
│   │   ├── add_embedding.txt
│   │   ├── correct_sentence copy.txt
│   │   ├── error.txt
│   │   ├── error4.txt
│   │   ├── initial_error.txt
│   │   ├── max_size32_error_sentence-1.txt
│   │   ├── max_size32_error_sentence-2.txt
│   │   ├── new_error_with_embedding-2.txt
│   │   ├── new_error_with_embedding-useless.txt
│   │   ├── record.txt
│   │   ├── test_结果记录.txt
│   │   ├── unwashed_data_max_size_32 copy.txt
│   │   ├── wash_BERT_lr=1e-5.txt
│   │   ├── wash_BERT_lr=2e-5.txt
│   │   ├── wash_BERT_lr=3e-5.txt
│   │   ├── wash_BERT_lr=4e-5.txt
│   │   └── wash_BERT_lr=5e-5.txt
│   ├── gpt结果.txt
│   ├── naive_bayes
│   │   └── res.txt
│   ├── nltk_sentiment
│   │   └── res.txt
│   ├── preprcessed_4.0.txt
│   ├── preprocessed_3.5.txt
│   ├── preprocessed_3.5_without_example.txt
│   ├── preprocessed_4.0_without_example.txt
│   ├── random_forest
│   │   ├── 2023-12-06_.log
│   │   └── 2023-12-17_.log
│   ├── raw_3.5.txt
│   ├── raw_4.0.txt
│   ├── svm
│   │   ├── 2023-12-06_.log
│   │   └── 2023-12-14.log
│   ├── useless
│   │   ├── 1e-5_myBERT_160000.txt
│   │   ├── 4e-5_256
│   │   ├── lr=1e-5_BERT.py测试结果.txt
│   │   ├── useless_ensemble
│   │   └── 加权
│   └── 结果list.txt
└── src
    ├── DL
    │   ├── BERT.py
    │   ├── BERT_class.py
    │   ├── BERTmy.py
    │   └── other_files
    ├── LLM
    │   ├── GPT.py
    │   └── openai-test.py
    ├── data
    │   ├── unwashed_data_max_size_32.txt
    │   ├── wash_data.ipynb
    │   ├── wash_data_advance.py
    │   └── wash_data_base.py
    └── traditional_ML
        ├── KNN.py
        ├── Naive_bayes.py
        ├── random_forest.py
        └── svm.py
```